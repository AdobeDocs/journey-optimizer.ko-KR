---
product: experience platform
solution: Experience Platform
title: AI 모델
description: 오퍼의 등급을 매길 수 있는 AI 모델에 대해 알아봅니다
feature: Ranking Formulas
role: User
level: Intermediate
exl-id: 4f7f7d1d-a12a-4ff6-b0ff-1a1c3d305a9d
source-git-commit: a7483965e3154d0ad34cfb56b6458bb63b46a26c
workflow-type: tm+mt
source-wordcount: '1530'
ht-degree: 0%

---

# AI 모델 {#ai-models}

## AI 모델 시작 {#get-started-with-ai-rankings}

지정된 프로필에 대해 표시할 오퍼에 등급을 지정하는 숙련된 모델 시스템을 사용할 수 있습니다.

>[!CAUTION]
>
>현재 AI 모델은 초기 액세스 시 사용자만 선택할 수 있도록 제공됩니다.

이 기능을 사용하면 다른 기능을 만들 수 있습니다 **AI 모델** 비즈니스 목표를 기반으로 구축 이러한 다양한 목표 기반 전략을 의사 결정에 사용하면 숙련된 모델 시스템을 통해 다양한 AI 모델이 목표에 미치는 영향을 파악할 수 있습니다.

예를 들어 이메일 채널에 대한 AI 모델과 푸시 채널에 대한 다른 모델을 선택할 수 있습니다. 각 채널에 대해, 훈련된 모델 시스템은 여러 데이터 포인트를 활용하여 오퍼의 우선 순위 점수나 오퍼를 고려하지 않고 지정된 배치에 대해 먼저 제공해야 하는 오퍼를 결정합니다 [등급 공식](create-ranking-formulas.md).

AI 모델이 만들어지면 결정에서 배치에 지정합니다. 추가 정보 [결정에서 오퍼 선택 구성](../offer-activities/configure-offer-selection.md).

>[!NOTE]
>
>현재 [!DNL Journey Optimizer] AI 등급에 대해 지원되는 모델 유형은 **자동 최적화**.

## 자동 최적화 모델 {#auto-optimization}

자동 최적화 모델은 비즈니스 클라이언트가 설정한 반환(KPI)을 극대화하는 오퍼를 제공하는 것을 목표로 합니다. 이러한 KPI는 전환율, 매출 등의 유형일 수 있습니다. 이때 자동 최적화는 오퍼를 Adobe Target으로 전환하여 오퍼 클릭 최적화에 중점을 둡니다. 자동 최적화는 오퍼의 &quot;전역&quot; 성능을 기반으로 개인화되지 않고 최적화합니다.

### 용어

다음 용어는 자동 최적화에 대해 논의할 때 유용합니다.

* **다중 무장 강도**: A [다중 무장 강도단](https://en.wikipedia.org/wiki/Multi-armed_bandit)최적화를 위한 {target=&quot;_blank&quot;} 접근 방식은 해당 학습의 탐구적 학습과 활용에 대한 균형을 맞춥니다.

* **톰슨 샘플링**: Thompson 샘플링은 즉각적인 성능을 최대화하기 위해 알려진 것을 이용하고 향후 성능을 향상시킬 수 있는 새로운 정보를 축적하기 위해 투자 간의 균형을 맞추기 위해 순차적으로 조치를 취하는 온라인 의사 결정 문제에 대한 알고리즘입니다. [자세히 알아보기](#thompson-sampling)

* [**베타 배포**](https://en.wikipedia.org/wiki/Beta_distribution){target=&quot;_blank&quot;}: 연속 집합 [확률 분포](https://en.wikipedia.org/wiki/Probability_distribution)간격에 정의된 {target=&quot;_blank&quot;} [0, 1] [매개 변수](https://en.wikipedia.org/wiki/Statistical_parameter){target=&quot;_blank&quot;}(두 개의 양수) [모양 매개 변수](https://en.wikipedia.org/wiki/Shape_parameter){target=&quot;_blank&quot;}.

### 톰슨 샘플링 {#thompson-sampling}

자동 최적화의 기반이 되는 알고리즘은 **톰슨 샘플링**. 이 섹션에서는 톰슨의 샘플에 대한 직관에 대해 토론합니다.

[톰슨 샘플링](https://en.wikipedia.org/wiki/Thompson_sampling){target=&quot;_blank&quot;} 또는 Bayesian 도적은 다중 무장 산적 문제에 대한 Bayesian 접근입니다.  기본적인 생각은 평균적인 보상을 다루는 ?? 각 오퍼에서 **임의 변수** 그리고 우리가 지금까지 수집한 데이터를 이용하여 평균적인 보상금에 대한 우리의 &quot;믿음&quot;을 갱신합니다. 이 &quot;믿음&quot;은 수학적으로 **후순위 확률 분포** - 기본적으로 각 오퍼에 대해 해당 값이 있는 성향(또는 가능성)과 함께 평균 포상금에 대한 값의 범위입니다. 그럼 모든 결정을 위해 **이 후기 보상 분배 각각에 대한 포인트 샘플** 그리고 샘플링된 보상이 가장 높은 오퍼를 선택합니다.

이 프로세스는 아래 그림에 나와 있으며, 여기에서는 3개의 다른 오퍼가 있습니다. 처음에 우리는 데이터로부터 어떤 증거도 가지고 있지 않으며, 우리는 모든 오퍼에 일관된 후순위 보상 분포가 있다고 가정합니다. 각 오퍼의 후순위 보상 분포에서 샘플을 추출합니다. 오퍼 2의 배포에서 선택한 샘플이 가장 높은 값을 갖습니다. 이것은 **탐험**. 오퍼 2를 표시한 후 잠재적 보상(예: 전환/전환 없음)을 수집하고 아래 설명된 대로 Bayes Theory를 사용하여 오퍼 2의 사후 배포를 업데이트합니다.  Adobe에서는 오퍼가 표시되고 보상이 수집될 때마다 이 프로세스를 계속 진행하고 후기 분배를 갱신합니다. 두 번째 그림에서 오퍼 3이 선택되었습니다. 평균 보상(후순위 보상 분포가 가장 오른쪽에 있음)이 가장 높은 오퍼 1에도 불구하고 각 배포의 샘플링 프로세스를 통해 명백히 최적 오퍼 3을 선택했습니다. 따라서 Adobe에서는 Offer 3의 실제 보상 분포에 대해 자세히 알아볼 수 있는 기회를 제공합니다.

더 많은 샘플이 수집되면 신뢰도가 높아지고 가능한 포상금에 대한 보다 정확한 추정이 획득됩니다(좁은 보상 분포에 해당). 더 많은 증거가 가능해짐에 따라 우리의 믿음을 업데이트하는 이러한 과정은 다음과 같이 알려져 있습니다 **베이시안 추론**.

결과적으로 한 오퍼(예: 오퍼 1)가 명백한 승자인 경우 해당 후순위 보상 분포가 다른 오퍼와 구분됩니다. 이 시점에서 각 결정에 대해, 오퍼 1에서 샘플링된 보상이 가장 높을 가능성이 있으며, 가능성이 더 높은 오퍼를 선택할 것입니다. 이것은 **개발** &quot; 저희는 오퍼 1이 가장 훌륭하다고 믿고 있습니다.따라서 보상을 극대화하기 위해 오퍼를 선택하게 되었습니다.&quot;

![](../assets/ai-ranking-thompson-sampling.png)

**그림 1**: *모든 결정에 대해, 우리는 후기 보상 분포에서 포인트를 샘플링합니다. 샘플 값이 가장 높은 오퍼(전환율)가 선택됩니다. 초기 단계에서는 데이터에서 오퍼의 전환율에 대한 증거가 없으므로 모든 오퍼에 균일한 분산이 있습니다. 우리가 더 많은 샘플을 수집함에 따라, 후기 분배는 더 좁고 더 정확해진다. 궁극적으로, 전환율이 가장 높은 오퍼가 매번 선택됩니다.*

<!--
![](../assets/ai-ranking-thompson-sampling-initial.png)
![](../assets/ai-ranking-thompson-sampling-intermediate.png)
![](../assets/ai-ranking-thompson-sampling-ultimate.png)
-->

+++**기술 세부 사항**

분배를 계산/갱신하려면 **베이스 정리**. 각 오퍼에 대해 ***i******P(??i)를 계산하려고 합니다. | data)***, 즉 각 오퍼에 대해 ***i***: 보상 값의 가능성 **??i** 은(는) 해당 오퍼에 대해 지금까지 수집한 데이터가 주어지면 됩니다.

베이스 정리:

***후기 = 가능성 * 이전***

다음 **이전 확률** 은 출력을 생성할 가능성에 대한 초기 추측입니다. 몇 가지 증거를 수집한 후, 그 확률을 **후기 확률**. 

자동 최적화는 이진 보상을 고려하도록 설계되었습니다(클릭/클릭 없음). 이 경우, 가능성은 N개의 시도 성공 횟수를 나타내며 **이종 분포**. 일부 가능성 기능의 경우, 특정 이전 을 선택하는 경우, 포스트레이터는 이전 과 동일한 배포로 끝납니다. 이러한 전의 는 **이전**. 이런 종류의 이전은 후기 분배의 계산을 매우 간단하게 합니다. 다음 **베타 배포** 바이너리 가능성(이진 보상) 이전의 접합체이므로 이전 및 후기 확률 분포에 편리하고 현명한 선택입니다.베타 배포는 두 가지 매개 변수를 사용합니다. ***알파*** 및 ***베타***. 이러한 매개 변수는 성공 및 실패 수 및 에 의해 제공되는 평균 값으로 생각할 수 있습니다.

![](../assets/ai-ranking-beta-distribution.png)

위에서 설명한 Positive 함수는 성공(전환) 및 실패(전환 없음)와 q가 있는 Binomial 분포를 통해 모델링됩니다 [임의 변수](https://en.wikipedia.org/wiki/Random_variable){target=&quot;_blank&quot;}( [베타 배포](https://en.wikipedia.org/wiki/Beta_distribution){target=&quot;_blank&quot;}.

이전 버전은 베타 배포로 모델링되며 후기 배포는 다음 양식을 사용합니다.

![](../assets/ai-ranking-posterior-distribution.svg)

포스트리터는 기존 매개 변수에 성공 및 실패 횟수를 추가하면 계산됩니다 ***알파***, ***베타***.

위의 예에 표시된 자동 최적화의 경우, Adobe에서는 이전 배포로 시작합니다 ***베타(1, 1)*** (균일한 배포) 모든 오퍼에 대해 그리고 지정된 오퍼에 대한 성공 및 실패를 받은 후, 포스트리저는 매개 변수를 사용한 베타 배포가 됩니다 ***(s+a, f+베타)*** 참조하십시오.
+++

**관련 항목**:

톰슨의 샘플에 대해 자세히 알아보려면 다음 연구 논문을 읽으세요.
* [톰슨샘플링 실증평가](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target=&quot;_blank&quot;}
* [다무장 산적문제를 위한 Thompson 시료 분석](http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target=&quot;_blank&quot;}

### 콜드 스타트 문제

새 오퍼가 캠페인에 추가되면 &quot;콜드 시작&quot; 문제가 발생하고, 새 오퍼의 전환율에 대해 사용할 수 있는 데이터가 없습니다. 이 기간 동안 이 새 오퍼의 전환율에 대한 정보를 수집하는 동안 성과 감소를 최소화하도록 이 새 오퍼를 선택하는 빈도에 대한 전략을 만들어야 합니다. 이 문제를 해결하는 데 사용할 수 있는 여러 가지 해결 방법이 있습니다. 중요한 것은 우리가 그 착취를 별로 희생하지 않고 이 새로운 오퍼의 탐구 사이에 균형을 찾는 것이다. 현재 Adobe에서는 새 오퍼의 전환율(이전 배포)에 대한 초기 추측으로 &quot;균일한 배포&quot;를 사용합니다. 기본적으로 모든 전환율 값을 발생 가능성과 동일하게 제공합니다.


![](../assets/ai-ranking-cold-start-strategies.png)

**그림 2**: *3개의 오퍼가 있는 캠페인을 고려하십시오. 캠페인이 라이브 상태인 동안 오퍼 4가 캠페인에 추가됩니다. 처음에는 오퍼 4의 전환율에 대한 데이터가 없으며 시작 미해결 문제를 해결해야 합니다. Adobe에서는 이 새 오퍼에 대한 데이터를 수집하는 동안 오퍼 4의 전환율에 대한 초기 추측으로 균일한 배포를 사용합니다. 에 설명된 대로 [톰슨 샘플링](#thompson-sampling) 섹션에서 사용자에게 표시할 오퍼를 선택하려면 오퍼의 후기 보상 배포에서 포인트를 샘플링하고 가장 높은 샘플 값이 있는 오퍼를 선택합니다. 위의 예에서, 오퍼 4는 수집된 리워드를 기반으로 하여 선택된 후 이 오퍼의 사후 배포는 다음에 설명된 대로 업데이트됩니다. [톰슨 샘플링](#thompson-sampling) 섹션을 참조하십시오.*

### 상승도 측정

상승도는 기준 전략(오퍼를 임의로 제공)과 비교하여 순위 서비스에 배포된 모든 전략의 성과를 측정하는 데 사용되는 지표입니다.

예를 들어, 순위 지정 서비스에서 사용되는 TS(Thompson Sampling) 전략의 성과를 측정하고 KPI가 전환율(CVR)인 경우 기준 전략에 대한 TS 전략의 &quot;상승도&quot;는 다음과 같이 정의됩니다.

![](../assets/ai-ranking-lift.png)
